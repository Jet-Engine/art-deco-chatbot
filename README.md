# Revamping the Art-Deco Bot: A Journey with RAG

## Introduction to RAG

Large Language Models (LLMs) have significantly advanced, improving their ability to answer a broad array of 
questions. However, they still encounter challenges, particularly with specific or recent information, 
often resulting in inaccuracies or "hallucinations." To address these issues, 
the Retrieval Augmented Generation (RAG) approach integrates a document retrieval step into the
response generation process. This approach uses a corpus of documents and employs vector databases 
for efficient retrieval, enhancing the accuracy and reliability of LLM responses through three key steps:

1. **Segmenting documents** into manageable chunks.
2. **Generating embeddings** for both the query and document chunks to measure their relevance through similarity scores.
3. **Retrieving the most relevant chunks** and using them as context to generate well-informed answers.

Vector databases facilitate quick similarity searches and efficient data management, making RAG a powerful 
solution for enhancing LLM capabilities.

## Purpose of the Art-Deco Bot

The Art-Deco era, spanning the 1920s to the 1940s, represents a significant chapter in architectural history. 
Despite the capabilities of models like Meta's Llama3, their responses can be unreliable, especially 
for nuanced or detailed queries specific to Art-Deco. Our goal with the Art-Deco Bot is to use RAG to improve 
the quality of responses about Art-Deco architecture, comparing these with those generated by traditional LLMs 
in both quality and time efficiency.

## Setting Up the Art-Deco Bot

### Prerequisites

#### Checking Matt Willians' RAG Project

We based our RAG project on Matt Williams' 
[https://github.com/technovangelist/videoprojects](Build RAG with Python)
project. The code that is taken from here is highly modified and extended. Before reading this blogpost and
understanding this project code we advise you to check 
Matt's project and related [https://www.youtube.com/watch?v=GxLoMquHynY](Youtube video).

#### Ollama

[https://ollama.com/](Ollama) is a program that facilitates running LLM models easily on local machines.

* Install Ollama on your local machine by following instructions on the [https://ollama.com/](Ollama website).
* Download the required models for the Art-Deco Bot project.
   * `ollama pull llama3` (LLM that would be used for RAG)
   * `ollama pull nomic-embed-text` (embedding model that would be used for RAG)
* You could run these models in your terminal after they are downloaded by using the commands above.
But having conversations with these models on your terminal screen is not a prerequisite for this project.

#### LiteLLM

In this project we not only aim to write code to show how RAG can be done but also to compare and bechmark results
of RAG with queries to different LLMs. Some of these LLMs could not be locally run like GPT-4. Some of them could 
be run locally but compute-heavy thus we choose to run them on cloud such as running Llama3:70b on Groq. 

In short we need to query different LLMs that have different Python libraries. One of the problems LiteLLM tries 
to solve is to provide a unified interface to query different LLMs. Although LiteLLM has many features, 
we will use it in our project for this purpose that would make our code cleaner and more readable. 

Checking [https://docs.litellm.ai/docs/](LiteLLM Python library) is not a prerequisite for this project 
but it is recommended.

#### API Keys

Get your API keys from [https://platform.openai.com]OpenAI and [https://console.groq.com] (Groq) to use them 
in the project.
Beware that you may be billed for using these services. Groq API could be used for free now 
but OpenAI API is not free.

#### Setting Up ChromaDB

[https://www.trychroma.com](ChromaDB) is a vector database that enables efficient storage and retrieval 
of document embeddings. To set up ChromaDB, follow these steps:

1. Install ChromaDB by running:
`pip install chromadb`
2. Start the ChromaDB server with:
`chroma run --host localhost --port 8000 --path INDEX_PATH`

You need to change `INDEX_PATH` with the path where you want to store the index data of ChromaDB.

#### Installing Dependencies

To get started, install all necessary dependencies by running:

`pip install -r requirements.txt`

### Running the Art-Deco Bot

Running the Art-Deco Bot involves several steps, including collecting documents, indexing them in a 
vector database, and querying the RAG model. Here's a detailed guide to help you navigate through the process.

#### Collecting Documents with `wiki-bot.py`

Our initial step involves gathering knowledge about Art-Deco architecture. We focus on U.S. structures, 
given their prominence in the Art-Deco movement. The `wiki-bot.py` script automates the collection of 
relevant Wikipedia articles, organizing them into a structured directory for ease of access.

Run the bot using:

`python wiki-bot.py`

The content files of all scraped articles are available in the `rag_files` directory, so there's no need to 
repeat the scraping process.

### Indexing Documents for Vector Database with `indexing.py`

Index the documents by running:

`python indexing.py`

Make sure ChromaDB is running before executing this script. 

### Using RAG with `chat.py`

Before running `chat.py`, ensure the ChromaDB server is active and the `config.yaml` settings are correct, 
including API keys for OpenAI and Groq.

Customize the queries by editing `questions.txt`. To initiate the Art-Deco Bot, run:

`python chat.py`

The bot will output answers in various formats (HTML, Markdown, JSON, CSV), allowing you to assess and 
compare the response quality of RAG and LLMs.

## Comparison of Responses

One of the aims of the Art-Deco Bot project is to compare the responses generated by RAG with those from 
traditional LLMs. 
By querying different models, we can evaluate their performance in terms of accuracy, relevance, 
and time efficiency.
Evaluation of the quality of responses is not easy and it is a subjective task. Quality of RAG responses are
also highly correlated with the quality of the documents that are indexed. 

Since we aim to experiment with different
embedding models and chunking techniques in future we skip a through evaluation of the quality of responses 
in this blogpost.

If you are interested you could compare results yourself by checking out generated tables that include 
responses and our document set that is indexed for RAG. Ironically we could outsource this task to LLM's 
such as GPT-4. We give our generated CSV files to GPT-4 and ask it to compare responses of RAG and LLMs. 
Below you could see the results:

```markdown
Analyzing the responses from the `ollama_rag` model compared to other LLMs (like GPT-4 and `ollama-llama3`) in your benchmark, we can make several observations regarding correctness, succinctness, and potential for hallucination. 
1. **Correctness:**  
- The `ollama_rag` model generally provides accurate answers similar to other models. For example, for the question about the opening of Radio City Music Hall, it correctly identifies the opening date as December 27, 1932, which matches the answers from GPT-4 and `groq-llama3-70b`. 
- However, there are instances where `ollama_rag` gives an incorrect or less accurate answer, such as the height of Rand Tower Hotel, where it provides an answer that lacks a specific figure, in contrast to the correct height given by `groq-llama3-70b`. 
2. **Succinctness:**  
- The `ollama_rag` responses tend to be more verbose compared to GPT-4. This model provides additional contextual information that might not be necessary to directly answer the question but enriches the user's understanding. For example, in describing the use of Mark Hellinger Theatre in its first decade, `ollama_rag` includes a detailed list of different uses, which is informative but more detailed than necessary for direct inquiries.
- This verbosity can be seen as a double-edged sword—it enhances detail at the cost of brevity, which may not always align with user expectations for succinctness. 
3. **Hallucination:**  
- The `ollama_rag` model seems to have issues with fabricating details or providing irrelevant historical context. For example, it mentioned details about different decades and events that were not strictly relevant to the direct use of the Mark Hellinger Theatre in its first decade. This suggests a tendency towards confabulation under certain conditions. 
- For questions where very specific or less well-known knowledge is required, such as the architectural details of Lamar High School, `ollama_rag` provides a blend of correct and possibly confabulated or less relevant details, which might mislead users who need precise information. 
4. **Comparative Performance:**  
- Against GPT-4 and other LLMs like `groq-llama3-70b` and `ollama-llama3`, `ollama_rag` holds up reasonably well in terms of factual accuracy but may lag in directness and clarity due to its verbose and occasionally less focused answers. 
- The `ollama_rag` responses suggest that while it integrates knowledge well, its application might be best suited for scenarios where detailed explorations of topics are more valuable than concise answers.

In summary, the `ollama_rag` model demonstrates a robust capability to generate detailed and contextually rich answers, but it may benefit from improvements in precision and adherence to the specific demands of queries to better align with user expectations for direct and succinct information.
```

## Comparison of Response Times

* Inference for LLama3 on RAG tasks take longer time than one sentence question inference on Llama3. 
This is expected since as number of tokens in queries increase, inference time increases.
* Indexing of document set takes considerable time. For example our Art-Deco document set contains 2109 plain
text files that is around 10MB in total. Indexing this document set with ChromaDB takes around 10 minutes on 
Mac Mini M2 Pro. For large document sets long indexing time may be a setback for RAG projects.
* Creating embeddings for queries and time taken for similarity search on vector database is negligible compared to LLM inference time.


## Roadmap for the Art-Deco Bot

In future blog posts, we plan to delve deeper into the Art-Deco Bot project.

* We would like to benchmark performance of different vector databases.
* We would like to add more questions to our question set.
* We would like to migrate the project on JetEngine's state of the art vector database PulseJet 
to make our bot more performant and scalable.
* We would like to explore different techniques and parameters for chunking and embedding similarity measurements.
* We would like to expand this project into different domains and LLMs with minimal code change.
* We would like to add GUI to the project to make it more user-friendly. 

Stay tuned for upcoming interesting stuff to get new insight about the exciting world of RAG and meanwhile
appreciating the beauty and elegance of Art-Deco architecture.

## Conclusion

This project not only underscores the potential of RAG in enhancing the reliability of LLMs but also invites enthusiasts and developers to explore the intriguing domain of Art-Deco architecture through advanced AI techniques.


**License**: MIT License

**Author**: Güvenç Usanmaz
