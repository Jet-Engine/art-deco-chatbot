# Revamping the Art-Deco Bot: A Journey with RAG

## Introduction to RAG

As Large Language Models (LLMs) evolve, their ability to answer a broad spectrum of questions improves significantly. However, they still face certain challenges, especially when dealing with very specific queries or recent events they haven't been trained on. This can result in inaccuracies and even confident misstatements, also known as "hallucinations."

To mitigate these issues, the **Retrieval Augmented Generation (RAG)** approach offers a powerful solution. By leveraging a corpus of documents and employing vector databases for efficient retrieval, RAG can enhance the accuracy and reliability of LLM responses. This method involves:

1. **Segmenting documents** into manageable chunks.
2. **Generating embeddings** for both the query and document chunks to measure their relevance through similarity scores.
3. **Retrieving the most relevant chunks** and using them as context to generate well-informed answers.

This process benefits greatly from vector databases, which facilitate quick similarity searches and efficient data management.

## Purpose of the Art-Deco Bot

The Art-Deco era, spanning from the 1920s to the 1940s, represents a fascinating chapter in architectural history. Despite the impressive capabilities of models like Meta's Llama3, their responses can sometimes be unreliable, particularly for nuanced or detailed queries specific to Art-Deco.

Our goal with the Art-Deco Bot is to use RAG to enhance the quality of responses about Art-Deco architecture. We aim to compare these responses with those generated by traditional LLMs, evaluating both their quality and time efficiency.

## Setting Up the Art-Deco Bot

### Installing Dependencies

To get started, install all necessary dependencies by running:


`pip install -r requirements.txt`

### Collecting Documents with `wiki-bot.py`

Our initial step involves gathering knowledge about Art-Deco architecture. We focus on U.S. structures, given their prominence in the Art-Deco movement. The `wiki-bot.py` script automates the collection of relevant Wikipedia articles, organizing them into a structured directory for ease of access.

Run the bot using:


`python wiki-bot.py`

The content files of all scraped articles are available in the `rag_files` directory, so there's no need to repeat the scraping process.

### Indexing Documents for Vector Database with `indexing.py`

Once the documents are collected, they must be indexed in a vector database. Start the ChromaDB server with:

bash

`chroma run --host localhost --port 8000 --path PATH_TO_INDEX_DATA_OF_CHROMA_DB`

Then, index the documents by running:


`python indexing.py`

### Using RAG with `chat.py`

Before running `chat.py`, ensure the ChromaDB server is active and the `config.yaml` settings are correct, including API keys for OpenAI and Groq. Install Ollama (visit [https://ollama.com/](https://ollama.com/) for installation details), and make sure the required models are available.

Customize the queries by editing `questions.txt`. To initiate the Art-Deco Bot, run:


`python chat.py`

The bot will output answers in various formats (HTML, Markdown, JSON, CSV), allowing you to assess and compare the response quality of RAG and LLMs.

## Conclusion

This project not only underscores the potential of RAG in enhancing the reliability of LLMs but also invites enthusiasts and developers to explore the intriguing domain of Art-Deco architecture through advanced AI techniques.

**License**: MIT License

**Author**: Güvenç Usanmaz
